{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f3f58d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from itertools import product\n",
    "\n",
    "import urllib\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f31bce",
   "metadata": {},
   "source": [
    "#### Create GoogleScraper Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3e5359",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Config Information \n",
    "SECRET_KEY = \"AIzaSyDjwQM14wUBM40e5xtL7Df3Qe4my03iuTA\"\n",
    "CSE_ID = \"e340b4c39a82947c4\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd22c7c3",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def count_indexed_pages(url):\n",
    "    \n",
    "    query = urllib.parse.quote_plus(url)\n",
    "\n",
    "\n",
    "    # Get the response\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(u\"https://www.google.com/search?q=site%3A\" + url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "    # Parse response\n",
    "    string = response.html.find(\"#result-stats\", first=True).text\n",
    "    indexed = int(string.split(' ')[1].replace(',',''))\n",
    "\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c69c7dd",
   "metadata": {
    "code_folding": [
     0,
     2,
     9,
     37,
     53,
     72,
     139,
     145,
     186
    ]
   },
   "outputs": [],
   "source": [
    "class GoogleScraper: \n",
    "    \n",
    "    def __init__(self, api_key: str, cse_id: str, headers: dict): \n",
    "        \n",
    "        # Grab args\n",
    "        self.api_key = api_key\n",
    "        self.cse_id = cse_id\n",
    "        self.headers = headers\n",
    "        \n",
    "    def search(self, keyword: str, skip_links: list = None): \n",
    "        \n",
    "        \"\"\"\n",
    "        Run the google search and get list of top 70 results.\n",
    "        -- \n",
    "        keyword: str -> phrase to search on google \n",
    "        skip_links: list -> list of links that we should skip such as \n",
    "        \"\"\"\n",
    "        \n",
    "        # Setup service operator \n",
    "        service = build(\"customsearch\", \"v1\", developerKey = self.api_key)\n",
    "        \n",
    "        # Make request \n",
    "        res_list = []\n",
    "        for start in [1, 11, 21, 31, 41, 51, 61]:\n",
    "            res = service.cse().list(q = keyword, cx = self.cse_id, start = start).execute()\n",
    "            results = [{k:v for k,v in res.items() if k in ['title', 'link', 'snippet']} for res in res['items']]\n",
    "            res_list.append(results)\n",
    "            \n",
    "        # Flatten List\n",
    "        res_list = [item for sublist in res_list for item in sublist]\n",
    "        \n",
    "        # Remove all that contain a skip link\n",
    "        res_list = [page for page in res_list if not any(substring in page['link'] for substring in skip_links)]\n",
    "        \n",
    "        \n",
    "        return res_list\n",
    "\n",
    "    def make_request(self, url): \n",
    "        \n",
    "        \"\"\"\n",
    "        Makes the GET request and uses BS4 to parse it\n",
    "        -- \n",
    "        url: string of the url to search\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make request with proper headers \n",
    "        req = requests.get(url, headers = self.headers)\n",
    "        \n",
    "        # Parse via BS4\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    def count_indexed_pages(url):\n",
    "    \n",
    "        query = urllib.parse.quote_plus(url)\n",
    "\n",
    "\n",
    "        # Get the response\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            response = session.get(u\"https://www.google.com/search?q=site%3A\" + url)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "\n",
    "        # Parse response\n",
    "        string = response.html.find(\"#result-stats\", first=True).text\n",
    "        indexed = int(string.split(' ')[1].replace(',',''))\n",
    "\n",
    "        return indexed\n",
    "\n",
    "    def get_word_count_info(self, soup): \n",
    "        \n",
    "        \"\"\"\n",
    "        Compute all the word count information we want from the parsed soup object. \n",
    "        Note that it removes any text in a header tag or footer tag\n",
    "        \"\"\"\n",
    "    \n",
    "        # Grab body text \n",
    "        body_text = re.sub(r'\\s+', ' ', soup.find('body').text).rstrip().lstrip()\n",
    "        body_text = re.sub(r'(?<=[.,?!])(?=[^\\s])', r' ', body_text).lower()\n",
    "        self.body_text = body_text.replace('\\n','')\n",
    "\n",
    "        # Grab header text\n",
    "        try: \n",
    "            header = soup.find('header')\n",
    "            header_text = re.sub(r'\\s+', ' ', header.text).rstrip().lstrip()\n",
    "            header_text = re.sub(r'(?<=[.,?!])(?=[^\\s])', r' ', header_text).lower()\n",
    "            header_text = header_text.replace('\\n','')\n",
    "            header_words = len(header_text.split())\n",
    "        except: \n",
    "            header_text = ''\n",
    "            header_words = 0\n",
    "\n",
    "\n",
    "        # Grab footer text \n",
    "        try: \n",
    "            footer = soup.find('footer')\n",
    "            footer_text = re.sub(r'\\s+', ' ', footer.text).rstrip().lstrip()\n",
    "            footer_text = re.sub(r'(?<=[.,?!])(?=[^\\s])', r' ', footer_text).lower()\n",
    "            footer_text = footer_text.replace('\\n','')\n",
    "            footer_words = len(footer_text.split())\n",
    "        except: \n",
    "            footer_text = ''\n",
    "            footer_words = 0\n",
    "\n",
    "\n",
    "        # Remove header/footer text\n",
    "        text = body_text.replace(header_text, '')\n",
    "        text = text.replace(footer_text, '').rstrip().lstrip()\n",
    "\n",
    "        # Compute word counts\n",
    "        total_words = len(text.split())\n",
    "\n",
    "        # Compute keyword frequency\n",
    "        keyword_freq = len(re.findall(r'car accident', text))\n",
    "        lawyer_freq = len(re.findall(r'lawyer|attorney', text))\n",
    "        city_freq = len(re.findall('los angeles', body_text))\n",
    "\n",
    "        # Combine the keyword into single word \n",
    "        text = text.replace('car accident', 'caraccident')\n",
    "\n",
    "        # Find first spot of keyword\n",
    "        first_keyword_spot = text.split().index('caraccident') + 1 if 'caraccident' in text.split() else 'NA'\n",
    "        keyword_in_200 = first_keyword_spot <= 200 if 'caraccident' in text.split() else False\n",
    "\n",
    "        word_count_info = {'total_words': total_words, \n",
    "                           'header_words': header_words, \n",
    "                           'footer_words': footer_words, \n",
    "                           'keyword_count': keyword_freq, \n",
    "                           'lawyer_attorney_count': lawyer_freq, \n",
    "                       'city_count': city_freq, \n",
    "                       'first_keyword_spot': first_keyword_spot, \n",
    "                       'keyword_in_200': keyword_in_200}\n",
    "    \n",
    "    \n",
    "        return word_count_info\n",
    "    \n",
    "    def get_alt(self, img): \n",
    "        try: \n",
    "            return img['alt']\n",
    "        except: \n",
    "            return 'NO ALT' \n",
    "    \n",
    "    def get_tag_info(self, soup): \n",
    "    \n",
    "        # Title tag info \n",
    "        title = soup.find('title').text\n",
    "        keyword_in_title = 'car accident' in title.lower()\n",
    "\n",
    "        # H1 info\n",
    "        h1 = soup.find_all('h1')\n",
    "        h1_count = len(h1)\n",
    "        h1_text = [el.text for el in h1]\n",
    "        keyword_in_h1 = ['car accident' in el.lower() for el in h1_text]\n",
    "\n",
    "        # H2 info\n",
    "        h2 = soup.find_all('h2')\n",
    "        h2_count = len(h2)\n",
    "        h2_text = [el.text for el in h2]\n",
    "        keyword_in_h2 = ['car accident' in el.lower() for el in h2_text]\n",
    "\n",
    "        # H3 info\n",
    "        h3 = soup.find_all('h3')\n",
    "        h3_count = len(h3)\n",
    "        h3_text = [el.text for el in h3]\n",
    "        keyword_in_h3 = ['car accident' in el.lower() for el in h3_text]\n",
    "\n",
    "        # Image info\n",
    "        images = soup.find_all('img')\n",
    "        image_count = len(images)\n",
    "        alt_text = [self.get_alt(img) for img in images]\n",
    "        keyword_in_alt_text = ['car accident' in el.lower() for el in alt_text]\n",
    "\n",
    "        # Combine \n",
    "        tag_info = {'title': title, 'key_in_title': keyword_in_title, \n",
    "                    'h1_count': h1_count, 'h1_text': h1_text, 'key_in_h1': keyword_in_h1, \n",
    "                    'h2_count': h2_count, 'h2_text': h2_text, 'key_in_h2': keyword_in_h2, \n",
    "                    'h3_count': h3_count, 'h3_text': h3_text, 'key_in_h3': keyword_in_h3, \n",
    "                    'image_count': image_count, 'image_alt_text': alt_text, \n",
    "                    'key_in_alt_text': keyword_in_alt_text\n",
    "                   }\n",
    "\n",
    "        return tag_info\n",
    "    \n",
    "    def get_link_info(self, soup): \n",
    "        \n",
    "        # Get word count info \n",
    "        info = self.get_word_count_info(soup)\n",
    "        \n",
    "        # Get tag info\n",
    "        tag_info = self.get_tag_info(soup)\n",
    "        info.update(tag_info)\n",
    "        \n",
    "        # Check if wordpress site\n",
    "        wp_script_tags = soup.find_all('link', {'href': re.compile(r'wp-content')})\n",
    "        info.update({'is_wordpress': bool(len(wp_script_tags) > 0)})\n",
    "        \n",
    "        # Check for table of contents\n",
    "        info.update({'has_table_of_contents': bool('table of contents' in self.body_text)})\n",
    "        \n",
    "        \n",
    "        \n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b1f4b",
   "metadata": {},
   "source": [
    "#### Grab Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81af37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_list = ['Personal Injury Lawyer',\n",
    "          'Car Accident Lawyer', \n",
    "          'Motorcycle Accident Lawyer', \n",
    "          'Truck Accident Lawyer', \n",
    "          'Medical Malpratice Lawyer', \n",
    "          'Wrongful Death Lawyer']\n",
    "city_list = ['Los Angeles', 'New York', 'Miami', 'Washington DC', 'Denver', 'Chicago']\n",
    "keyword_list = [f'{k} {city}' for k in k_list for city in city_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4860c614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Personal Injury Lawyer Los Angeles',\n",
       " 'Personal Injury Lawyer New York',\n",
       " 'Personal Injury Lawyer Miami',\n",
       " 'Personal Injury Lawyer Washington DC',\n",
       " 'Personal Injury Lawyer Denver',\n",
       " 'Personal Injury Lawyer Chicago',\n",
       " 'Car Accident Lawyer Los Angeles',\n",
       " 'Car Accident Lawyer New York',\n",
       " 'Car Accident Lawyer Miami',\n",
       " 'Car Accident Lawyer Washington DC',\n",
       " 'Car Accident Lawyer Denver',\n",
       " 'Car Accident Lawyer Chicago',\n",
       " 'Motorcycle Accident Lawyer Los Angeles',\n",
       " 'Motorcycle Accident Lawyer New York',\n",
       " 'Motorcycle Accident Lawyer Miami',\n",
       " 'Motorcycle Accident Lawyer Washington DC',\n",
       " 'Motorcycle Accident Lawyer Denver',\n",
       " 'Motorcycle Accident Lawyer Chicago',\n",
       " 'Truck Accident Lawyer Los Angeles',\n",
       " 'Truck Accident Lawyer New York',\n",
       " 'Truck Accident Lawyer Miami',\n",
       " 'Truck Accident Lawyer Washington DC',\n",
       " 'Truck Accident Lawyer Denver',\n",
       " 'Truck Accident Lawyer Chicago',\n",
       " 'Medical Malpratice Lawyer Los Angeles',\n",
       " 'Medical Malpratice Lawyer New York',\n",
       " 'Medical Malpratice Lawyer Miami',\n",
       " 'Medical Malpratice Lawyer Washington DC',\n",
       " 'Medical Malpratice Lawyer Denver',\n",
       " 'Medical Malpratice Lawyer Chicago',\n",
       " 'Wrongful Death Lawyer Los Angeles',\n",
       " 'Wrongful Death Lawyer New York',\n",
       " 'Wrongful Death Lawyer Miami',\n",
       " 'Wrongful Death Lawyer Washington DC',\n",
       " 'Wrongful Death Lawyer Denver',\n",
       " 'Wrongful Death Lawyer Chicago']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27c00a57",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n",
      "Page 3 of 10 complete\n",
      "Page 4 of 10 complete\n",
      "Page 5 of 10 complete\n",
      "Page 6 of 10 complete\n",
      "Page 7 of 10 complete\n",
      "Page 8 of 10 complete\n",
      "Page 9 of 10 complete\n",
      "Page 10 of 10 complete\n",
      "--Motorcycle Accident Lawyer Los Angeles COMPLETE--\n",
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n",
      "Page 3 of 10 complete\n",
      "Page 4 of 10 complete\n",
      "Page 5 of 10 complete\n",
      "Page 6 of 10 complete\n",
      "Page 7 of 10 complete\n",
      "Page 8 of 10 complete\n",
      "Page 9 of 10 complete\n",
      "Page 10 of 10 complete\n",
      "--Motorcycle Accident Lawyer New York COMPLETE--\n",
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n",
      "Page 3 of 10 complete\n",
      "Page 4 of 10 complete\n",
      "Page 5 of 10 complete\n",
      "Page 6 of 10 complete\n",
      "Page 7 of 10 complete\n",
      "Page 8 of 10 complete\n",
      "Page 9 of 10 complete\n",
      "Page 10 of 10 complete\n",
      "--Motorcycle Accident Lawyer Miami COMPLETE--\n",
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n",
      "Page 3 of 10 complete\n",
      "Page 4 of 10 complete\n",
      "Page 5 of 10 complete\n",
      "Page 6 of 10 complete\n",
      "Page 7 of 10 complete\n",
      "Page 8 of 10 complete\n",
      "Page 9 of 10 complete\n",
      "Page 10 of 10 complete\n",
      "--Motorcycle Accident Lawyer Washington DC COMPLETE--\n",
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n",
      "Page 3 of 10 complete\n",
      "Page 4 of 10 complete\n",
      "Page 5 of 10 complete\n",
      "Page 6 of 10 complete\n",
      "Page 7 of 10 complete\n",
      "Page 8 of 10 complete\n",
      "Page 9 of 10 complete\n",
      "Page 10 of 10 complete\n",
      "--Motorcycle Accident Lawyer Denver COMPLETE--\n",
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n",
      "Page 3 of 10 complete\n",
      "Page 4 of 10 complete\n",
      "Page 5 of 10 complete\n",
      "Page 6 of 10 complete\n",
      "Page 7 of 10 complete\n",
      "Page 8 of 10 complete\n",
      "Page 9 of 10 complete\n",
      "Page 10 of 10 complete\n",
      "--Motorcycle Accident Lawyer Chicago COMPLETE--\n",
      "Page 1 of 10 complete\n",
      "Page 2 of 10 complete\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m soup \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mmake_request(res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Parse information\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_link_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m info\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrank\u001b[39m\u001b[38;5;124m'\u001b[39m: idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m     23\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(info)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mGoogleScraper.get_link_info\u001b[0;34m(self, soup)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_link_info\u001b[39m(\u001b[38;5;28mself\u001b[39m, soup): \n\u001b[1;32m    188\u001b[0m     \n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Get word count info \u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_word_count_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# Get tag info\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     tag_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_tag_info(soup)\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mGoogleScraper.get_word_count_info\u001b[0;34m(self, soup)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;03mCompute all the word count information we want from the parsed soup object. \u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03mNote that it removes any text in a header tag or footer tag\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Grab body text \u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m body_text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m)\u001b[38;5;241m.\u001b[39mrstrip()\u001b[38;5;241m.\u001b[39mlstrip()\n\u001b[1;32m     82\u001b[0m body_text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(?<=[.,?!])(?=[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms])\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, body_text)\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbody_text \u001b[38;5;241m=\u001b[39m body_text\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "# Init scraper\n",
    "scraper = GoogleScraper(SECRET_KEY, CSE_ID, HEADERS)\n",
    "skip_links = ['yelp.com', 'forbes.com', \n",
    "              'expertise.com', 'findlaw.com', \n",
    "              'superlawyers.com', 'martindale.com', \n",
    "              'justia.com', ]\n",
    "\n",
    "for keyword in keyword_list[12:]: \n",
    "    \n",
    "    # Get top 70 links\n",
    "    res_list = scraper.search(keyword, skip_links)\n",
    "    \n",
    "    data = []\n",
    "    for idx, res in enumerate(res_list[:10]): \n",
    "\n",
    "        # Make the get request\n",
    "        soup = scraper.make_request(res['link'])\n",
    "\n",
    "        # Parse information\n",
    "        info = scraper.get_link_info(soup)\n",
    "        info.update({'rank': idx + 1, 'url': res['link']})\n",
    "\n",
    "        data.append(info)\n",
    "\n",
    "        print(f\"Page {idx+1} of {10} complete\")\n",
    "        \n",
    "    # Convert to df\n",
    "    df = pd.DataFrame(data)\n",
    "    df['keyword'] = keyword\n",
    "    df['performer'] = df['rank'] <= 10\n",
    "    cols = ['keyword', 'rank', 'performer', 'url']\n",
    "    df = df[cols + [col for col in df if col not in cols]]\n",
    "    df.to_csv(f'{keyword}.csv')\n",
    "    print(f'--{keyword} COMPLETE--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c1419fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(city_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caab257",
   "metadata": {},
   "source": [
    "#### Extract Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f5d39a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 of 10\n",
      "Page 1 of 10\n",
      "Page 2 of 10\n",
      "Page 3 of 10\n",
      "Page 4 of 10\n",
      "Page 5 of 10\n",
      "Page 6 of 10\n",
      "Page 7 of 10\n",
      "Page 8 of 10\n",
      "Page 9 of 10\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for idx, res in enumerate(res_list[:10]): \n",
    "    \n",
    "    # Make the get request\n",
    "    soup = scraper.make_request(res['link'])\n",
    "    \n",
    "    # Parse information\n",
    "    info = scraper.get_link_info(soup)\n",
    "    info.update({'rank': idx + 1, 'url': res['link']})\n",
    "    \n",
    "    data.append(info)\n",
    "    \n",
    "    print(f\"Page {idx+1} of {10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "563f6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df['keyword'] = keyword\n",
    "df['performer'] = df['rank'] <= 10\n",
    "cols = ['keyword', 'rank', 'performer', 'url']\n",
    "df = df[cols + [col for col in df if col not in cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c80dfb75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6bfaff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('test_matador_scrape.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef94df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f3f58d",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import urllib\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f31bce",
   "metadata": {},
   "source": [
    "#### Create GoogleScraper Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc3e5359",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Config Information \n",
    "SECRET_KEY = \"AIzaSyDjwQM14wUBM40e5xtL7Df3Qe4my03iuTA\"\n",
    "CSE_ID = \"e340b4c39a82947c4\"\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd22c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_indexed_pages(url):\n",
    "    \n",
    "    query = urllib.parse.quote_plus(url)\n",
    "\n",
    "\n",
    "    # Get the response\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(u\"https://www.google.com/search?q=site%3A\" + url)\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "\n",
    "    # Parse response\n",
    "    string = response.html.find(\"#result-stats\", first=True).text\n",
    "    indexed = int(string.split(' ')[1].replace(',',''))\n",
    "\n",
    "    return indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5773319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_indexed_pages('https://marketresearch.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3c69c7dd",
   "metadata": {
    "code_folding": [
     2,
     9,
     37,
     72,
     139,
     145,
     186
    ]
   },
   "outputs": [],
   "source": [
    "class GoogleScraper: \n",
    "    \n",
    "    def __init__(self, api_key: str, cse_id: str, headers: dict): \n",
    "        \n",
    "        # Grab args\n",
    "        self.api_key = api_key\n",
    "        self.cse_id = cse_id\n",
    "        self.headers = headers\n",
    "        \n",
    "    def search(self, keyword: str, skip_links: list = None): \n",
    "        \n",
    "        \"\"\"\n",
    "        Run the google search and get list of top 70 results.\n",
    "        -- \n",
    "        keyword: str -> phrase to search on google \n",
    "        skip_links: list -> list of links that we should skip such as \n",
    "        \"\"\"\n",
    "        \n",
    "        # Setup service operator \n",
    "        service = build(\"customsearch\", \"v1\", developerKey = self.api_key)\n",
    "        \n",
    "        # Make request \n",
    "        res_list = []\n",
    "        for start in [1, 11, 21, 31, 41, 51, 61]:\n",
    "            res = service.cse().list(q = keyword, cx = self.cse_id, start = start).execute()\n",
    "            results = [{k:v for k,v in res.items() if k in ['title', 'link', 'snippet']} for res in res['items']]\n",
    "            res_list.append(results)\n",
    "            \n",
    "        # Flatten List\n",
    "        res_list = [item for sublist in res_list for item in sublist]\n",
    "        \n",
    "        # Remove all that contain a skip link\n",
    "        res_list = [page for page in res_list if not any(substring in page['link'] for substring in skip_links)]\n",
    "        \n",
    "        \n",
    "        return res_list\n",
    "\n",
    "    def make_request(self, url): \n",
    "        \n",
    "        \"\"\"\n",
    "        Makes the GET request and uses BS4 to parse it\n",
    "        -- \n",
    "        url: string of the url to search\n",
    "        \"\"\"\n",
    "        \n",
    "        # Make request with proper headers \n",
    "        req = requests.get(url, headers = self.headers)\n",
    "        \n",
    "        # Parse via BS4\n",
    "        soup = BeautifulSoup(req.content, 'html.parser')\n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    def count_indexed_pages(url):\n",
    "    \n",
    "        query = urllib.parse.quote_plus(url)\n",
    "\n",
    "\n",
    "        # Get the response\n",
    "        try:\n",
    "            session = HTMLSession()\n",
    "            response = session.get(u\"https://www.google.com/search?q=site%3A\" + url)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(e)\n",
    "\n",
    "        # Parse response\n",
    "        string = response.html.find(\"#result-stats\", first=True).text\n",
    "        indexed = int(string.split(' ')[1].replace(',',''))\n",
    "\n",
    "        return indexed\n",
    "\n",
    "    def get_word_count_info(self, soup): \n",
    "        \n",
    "        \"\"\"\n",
    "        Compute all the word count information we want from the parsed soup object. \n",
    "        Note that it removes any text in a header tag or footer tag\n",
    "        \"\"\"\n",
    "    \n",
    "        # Grab body text \n",
    "        body_text = re.sub(r'\\s+', ' ', soup.find('body').text).rstrip().lstrip()\n",
    "        body_text = re.sub(r'(?<=[.,?!])(?=[^\\s])', r' ', body_text).lower()\n",
    "        body_text = body_text.replace('\\n','')\n",
    "\n",
    "        # Grab header text\n",
    "        try: \n",
    "            header = soup.find('header')\n",
    "            header_text = re.sub(r'\\s+', ' ', header.text).rstrip().lstrip()\n",
    "            header_text = re.sub(r'(?<=[.,?!])(?=[^\\s])', r' ', header_text).lower()\n",
    "            header_text = header_text.replace('\\n','')\n",
    "            header_words = len(header_text.split())\n",
    "        except: \n",
    "            header_text = ''\n",
    "            header_words = 0\n",
    "\n",
    "\n",
    "        # Grab footer text \n",
    "        try: \n",
    "            footer = soup.find('footer')\n",
    "            footer_text = re.sub(r'\\s+', ' ', footer.text).rstrip().lstrip()\n",
    "            footer_text = re.sub(r'(?<=[.,?!])(?=[^\\s])', r' ', footer_text).lower()\n",
    "            footer_text = footer_text.replace('\\n','')\n",
    "            footer_words = len(footer_text.split())\n",
    "        except: \n",
    "            footer_text = ''\n",
    "            footer_words = 0\n",
    "\n",
    "\n",
    "        # Remove header/footer text\n",
    "        text = body_text.replace(header_text, '')\n",
    "        text = text.replace(footer_text, '').rstrip().lstrip()\n",
    "\n",
    "        # Compute word counts\n",
    "        total_words = len(text.split())\n",
    "\n",
    "        # Compute keyword frequency\n",
    "        keyword_freq = len(re.findall(r'car accident', text))\n",
    "        lawyer_freq = len(re.findall(r'lawyer|attorney', text))\n",
    "        city_freq = len(re.findall('los angeles', body_text))\n",
    "\n",
    "        # Combine the keyword into single word \n",
    "        text = text.replace('car accident', 'caraccident')\n",
    "\n",
    "        # Find first spot of keyword\n",
    "        first_keyword_spot = text.split().index('caraccident') + 1 if 'caraccident' in text.split() else 'NA'\n",
    "        keyword_in_200 = first_keyword_spot <= 200 if 'caraccident' in text.split() else False\n",
    "\n",
    "        word_count_info = {'total_words': total_words, \n",
    "                           'header_words': header_words, \n",
    "                           'footer_words': footer_words, \n",
    "                           'keyword_count': keyword_freq, \n",
    "                           'lawyer_attorney_count': lawyer_freq, \n",
    "                       'city_count': city_freq, \n",
    "                       'first_keyword_spot': first_keyword_spot, \n",
    "                       'keyword_in_200': keyword_in_200}\n",
    "    \n",
    "    \n",
    "        return word_count_info\n",
    "    \n",
    "    def get_alt(self, img): \n",
    "        try: \n",
    "            return img['alt']\n",
    "        except: \n",
    "            return 'NO ALT' \n",
    "    \n",
    "    def get_tag_info(self, soup): \n",
    "    \n",
    "        # Title tag info \n",
    "        title = soup.find('title').text\n",
    "        keyword_in_title = 'car accident' in title.lower()\n",
    "\n",
    "        # H1 info\n",
    "        h1 = soup.find_all('h1')\n",
    "        h1_count = len(h1)\n",
    "        h1_text = [el.text for el in h1]\n",
    "        keyword_in_h1 = ['car accident' in el.lower() for el in h1_text]\n",
    "\n",
    "        # H2 info\n",
    "        h2 = soup.find_all('h2')\n",
    "        h2_count = len(h2)\n",
    "        h2_text = [el.text for el in h2]\n",
    "        keyword_in_h2 = ['car accident' in el.lower() for el in h2_text]\n",
    "\n",
    "        # H3 info\n",
    "        h3 = soup.find_all('h3')\n",
    "        h3_count = len(h3)\n",
    "        h3_text = [el.text for el in h3]\n",
    "        keyword_in_h3 = ['car accident' in el.lower() for el in h3_text]\n",
    "\n",
    "        # Image info\n",
    "        images = soup.find_all('img')\n",
    "        image_count = len(images)\n",
    "        alt_text = [self.get_alt(img) for img in img_list]\n",
    "        keyword_in_alt_text = ['car accident' in el.lower() for el in alt_text]\n",
    "\n",
    "        # Combine \n",
    "        tag_info = {'title': title, 'key_in_title': keyword_in_title, \n",
    "                    'h1_count': h1_count, 'h1_text': h1_text, 'key_in_h1': keyword_in_h1, \n",
    "                    'h2_count': h2_count, 'h2_text': h2_text, 'key_in_h2': keyword_in_h2, \n",
    "                    'h3_count': h3_count, 'h3_text': h3_text, 'key_in_h3': keyword_in_h3, \n",
    "                    'image_count': image_count, 'image_alt_text': alt_text, \n",
    "                    'key_in_alt_text': keyword_in_alt_text\n",
    "                   }\n",
    "\n",
    "        return tag_info\n",
    "    \n",
    "    def get_link_info(self, soup): \n",
    "        \n",
    "        # Get word count info \n",
    "        info = self.get_word_count_info(soup)\n",
    "        \n",
    "        # Get tag info\n",
    "        tag_info = self.get_tag_info(soup)\n",
    "        info.update(tag_info)\n",
    "        \n",
    "        # Check if wordpress site\n",
    "        wp_script_tags = soup.find_all('link', {'href': re.compile(r'wp-content')})\n",
    "        info.update({'is_wordpress': bool(len(wp_script_tags) > 0)})\n",
    "        \n",
    "        # Check for table of contents\n",
    "        info.update({'has_table_of_contents': bool('table of contents' in body_text)})\n",
    "        \n",
    "        \n",
    "        \n",
    "        return info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3b1f4b",
   "metadata": {},
   "source": [
    "#### Grab Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "27c00a57",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=car+accident+lawyer+Los+Angeles&cx=e340b4c39a82947c4&start=11&key=AIzaSyDjwQM14wUBM40e5xtL7Df3Qe4my03iuTA&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:182610990967'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:182610990967'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [272]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m skip_links \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp.com\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforbes.com\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpertise.com\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfindlaw.com\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuperlawyers.com\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get list of top 70 links\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m res_list \u001b[38;5;241m=\u001b[39m \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_links\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [271]\u001b[0m, in \u001b[0;36mGoogleScraper.search\u001b[0;34m(self, keyword, skip_links)\u001b[0m\n\u001b[1;32m     23\u001b[0m res_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m start \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m11\u001b[39m, \u001b[38;5;241m21\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m41\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m61\u001b[39m]:\n\u001b[0;32m---> 25\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mservice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkeyword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcse_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     results \u001b[38;5;241m=\u001b[39m [{k:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m res\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m]} \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitems\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     27\u001b[0m     res_list\u001b[38;5;241m.\u001b[39mappend(results)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/googleapiclient/_helpers.py:130\u001b[0m, in \u001b[0;36mpositional.<locals>.positional_decorator.<locals>.positional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m positional_parameters_enforcement \u001b[38;5;241m==\u001b[39m POSITIONAL_WARNING:\n\u001b[1;32m    129\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/googleapiclient/http.py:938\u001b[0m, in \u001b[0;36mHttpRequest.execute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    936\u001b[0m     callback(resp)\n\u001b[1;32m    937\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpError(resp, content, uri\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muri)\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostproc(resp, content)\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 429 when requesting https://customsearch.googleapis.com/customsearch/v1?q=car+accident+lawyer+Los+Angeles&cx=e340b4c39a82947c4&start=11&key=AIzaSyDjwQM14wUBM40e5xtL7Df3Qe4my03iuTA&alt=json returned \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:182610990967'.\". Details: \"[{'message': \"Quota exceeded for quota metric 'Queries' and limit 'Queries per day' of service 'customsearch.googleapis.com' for consumer 'project_number:182610990967'.\", 'domain': 'global', 'reason': 'rateLimitExceeded'}]\">"
     ]
    }
   ],
   "source": [
    "# Init scraper\n",
    "scraper = GoogleScraper(SECRET_KEY, CSE_ID, HEADERS)\n",
    "\n",
    "# Define search word + links to skip\n",
    "keyword = 'car accident lawyer Los Angeles'\n",
    "skip_links = ['yelp.com', 'forbes.com', 'expertise.com', 'findlaw.com', 'superlawyers.com']\n",
    "\n",
    "# Get list of top 70 links\n",
    "res_list = scraper.search(keyword, skip_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caab257",
   "metadata": {},
   "source": [
    "#### Extract Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9f5d39a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 0 of 10\n",
      "Page 1 of 10\n",
      "Page 2 of 10\n",
      "Page 3 of 10\n",
      "Page 4 of 10\n",
      "Page 5 of 10\n",
      "Page 6 of 10\n",
      "Page 7 of 10\n",
      "Page 8 of 10\n",
      "Page 9 of 10\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for idx, res in enumerate(res_list[:10]): \n",
    "    \n",
    "    # Make the get request\n",
    "    soup = scraper.make_request(res['link'])\n",
    "    \n",
    "    # Parse information\n",
    "    info = scraper.get_link_info(soup)\n",
    "    info.update({'rank': idx + 1, 'url': res['link']})\n",
    "    \n",
    "    data.append(info)\n",
    "    \n",
    "    print(f\"Page {idx} of {10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "563f6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df['keyword'] = keyword\n",
    "df['performer'] = df['rank'] <= 10\n",
    "cols = ['keyword', 'rank', 'performer', 'url']\n",
    "df = df[cols + [col for col in df if col not in cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6bfaff64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('test_matador_scrape.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef94df7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
